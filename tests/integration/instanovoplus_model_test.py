from __future__ import annotations

import math
from typing import Any

import numpy as np
import pandas as pd
import polars as pl
import pytest
import torch
from datasets.arrow_dataset import Dataset
from datasets.dataset_dict import DatasetDict
from datasets.dataset_dict import IterableDatasetDict
from datasets.iterable_dataset import IterableDataset
from torch.utils.data import DataLoader

from instanovo.diffusion.dataset import AnnotatedPolarsSpectrumDataset
from instanovo.diffusion.dataset import collate_batches
from instanovo.diffusion.multinomial_diffusion import MultinomialDiffusion
from instanovo.inference.diffusion import DiffusionDecoder
from instanovo.transformer.dataset import collate_batch
from instanovo.transformer.dataset import SpectrumDataset
from instanovo.utils.metrics import Metrics
from tests.conftest import reset_seed


@pytest.mark.usefixtures("_reset_seed")
def test_model(
    instanovoplus_model: tuple[MultinomialDiffusion, DiffusionDecoder],
    load_preds: list[str],
    dataset: DatasetDict | Dataset | IterableDatasetDict | IterableDataset,
    instanovo_model: tuple[Any, Any],
) -> None:
    """Test loading an InstaNovo+ model and doing inference end-to-end."""
    diffusion_model, diffusion_decoder = instanovoplus_model

    diffusion_dataset = AnnotatedPolarsSpectrumDataset(
        pl.from_pandas(pd.DataFrame(dataset)), peptides=load_preds
    )
    assert len(diffusion_dataset) == 271

    spectrum, precursor_mz, precursor_charge, peptide = diffusion_dataset[0]

    assert torch.allclose(
        spectrum,
        torch.tensor(
            [
                [1.0096e02, 6.4273e-03],
                [1.1006e02, 6.0129e-03],
                [1.1646e02, 5.7488e-03],
                [1.2910e02, 2.5723e-02],
                [1.3009e02, 2.5281e-02],
                [1.4711e02, 3.0318e-02],
                [1.7309e02, 6.7769e-03],
                [1.8612e02, 1.3652e-02],
                [2.0413e02, 2.9710e-02],
                [2.7303e02, 7.5393e-03],
                [2.8318e02, 1.7115e-02],
                [3.0119e02, 3.4304e-01],
                [3.2845e02, 8.4199e-03],
                [3.7222e02, 4.9525e-02],
                [4.7877e02, 7.6899e-03],
                [5.2873e02, 9.7643e-03],
                [5.7176e02, 1.0661e-02],
                [5.7975e02, 1.1832e-02],
                [6.1527e02, 9.5337e-03],
                [6.5630e02, 1.2352e-02],
                [7.7837e02, 2.2391e-02],
                [7.7887e02, 7.0299e-02],
                [7.7938e02, 5.1397e-03],
                [7.9137e02, 1.6256e-02],
                [7.9189e02, 1.6157e-02],
                [8.0088e02, 6.5557e-02],
                [1.0365e03, 1.1454e-02],
                [1.1015e03, 1.2202e-02],
                [1.1395e03, 5.5224e-02],
                [1.1895e03, 1.2289e-02],
                [1.2285e03, 2.2203e-02],
                [1.2556e03, 1.8892e-02],
                [1.2565e03, 2.0319e-02],
                [1.2716e03, 1.0596e-01],
                [1.2996e03, 3.2774e-01],
                [1.5998e03, 1.0000e00],
            ]
        ),
        rtol=1e-04,
    )
    assert precursor_mz == 800.38427734375
    assert precursor_charge == 2.0
    assert peptide == "NRNVGDQNGC(+57.02)LAPGK"

    diffusion_data_loader = DataLoader(
        diffusion_dataset,
        batch_size=3,
        shuffle=False,
        collate_fn=collate_batches(
            residues=diffusion_model.residues,
            max_length=diffusion_model.config.max_length,
            time_steps=diffusion_decoder.time_steps,
            annotated=True,
        ),
    )

    batch = next(iter(diffusion_data_loader))
    spectra, spectra_padding_mask, precursors, peptides, peptide_padding_mask = batch

    assert torch.allclose(
        spectra,
        torch.tensor(
            [
                [
                    [1.0096e02, 6.4273e-03],
                    [1.1006e02, 6.0129e-03],
                    [1.1646e02, 5.7488e-03],
                    [1.2910e02, 2.5723e-02],
                    [1.3009e02, 2.5281e-02],
                    [1.4711e02, 3.0318e-02],
                    [1.7309e02, 6.7769e-03],
                    [1.8612e02, 1.3652e-02],
                    [2.0413e02, 2.9710e-02],
                    [2.7303e02, 7.5393e-03],
                    [2.8318e02, 1.7115e-02],
                    [3.0119e02, 3.4304e-01],
                    [3.2845e02, 8.4199e-03],
                    [3.7222e02, 4.9525e-02],
                    [4.7877e02, 7.6899e-03],
                    [5.2873e02, 9.7643e-03],
                    [5.7176e02, 1.0661e-02],
                    [5.7975e02, 1.1832e-02],
                    [6.1527e02, 9.5337e-03],
                    [6.5630e02, 1.2352e-02],
                    [7.7837e02, 2.2391e-02],
                    [7.7887e02, 7.0299e-02],
                    [7.7938e02, 5.1397e-03],
                    [7.9137e02, 1.6256e-02],
                    [7.9189e02, 1.6157e-02],
                    [8.0088e02, 6.5557e-02],
                    [1.0365e03, 1.1454e-02],
                    [1.1015e03, 1.2202e-02],
                    [1.1395e03, 5.5224e-02],
                    [1.1895e03, 1.2289e-02],
                    [1.2285e03, 2.2203e-02],
                    [1.2556e03, 1.8892e-02],
                    [1.2565e03, 2.0319e-02],
                    [1.2716e03, 1.0596e-01],
                    [1.2996e03, 3.2774e-01],
                    [1.5998e03, 1.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                ],
                [
                    [1.0206e02, 1.5187e-01],
                    [1.0637e02, 6.3501e-03],
                    [1.1509e02, 7.8075e-03],
                    [1.2709e02, 8.3823e-03],
                    [1.2910e02, 4.5442e-02],
                    [1.3005e02, 6.3955e-03],
                    [1.3009e02, 2.9635e-02],
                    [1.4510e02, 8.2660e-03],
                    [1.4711e02, 5.3368e-02],
                    [1.5508e02, 3.6658e-02],
                    [1.6910e02, 1.2327e-02],
                    [1.7111e02, 6.5105e-02],
                    [1.7309e02, 6.3960e-02],
                    [1.8509e02, 9.3112e-03],
                    [1.8612e02, 9.3916e-03],
                    [1.9508e02, 1.0712e-02],
                    [1.9911e02, 3.1394e-02],
                    [2.0109e02, 3.8500e-02],
                    [2.0413e02, 7.2634e-02],
                    [2.1309e02, 2.8069e-02],
                    [2.3110e02, 1.7229e-02],
                    [2.3882e02, 6.7135e-03],
                    [2.3909e02, 1.0725e-02],
                    [2.3912e02, 6.6401e-03],
                    [2.4108e02, 1.6571e-02],
                    [2.4413e02, 7.9794e-03],
                    [2.5329e02, 7.0339e-03],
                    [2.5909e02, 3.4415e-02],
                    [2.7266e02, 9.3057e-03],
                    [2.8318e02, 5.7852e-02],
                    [2.8412e02, 1.5341e-02],
                    [2.9965e02, 7.4997e-03],
                    [3.0119e02, 1.0000e00],
                    [3.1365e02, 2.9935e-02],
                    [3.2216e02, 1.6580e-02],
                    [3.2412e02, 9.2876e-03],
                    [3.2816e02, 1.1835e-02],
                    [3.2865e02, 4.4182e-02],
                    [3.4213e02, 1.4129e-02],
                    [3.4217e02, 8.9981e-03],
                    [3.5017e02, 4.1863e-02],
                    [3.5421e02, 1.7921e-02],
                    [3.5517e02, 6.6786e-02],
                    [3.7222e02, 1.7619e-01],
                    [3.7669e02, 1.6286e-02],
                    [3.8569e02, 3.6224e-02],
                    [3.9516e02, 9.7139e-03],
                    [4.1317e02, 3.4127e-02],
                    [4.1419e02, 1.5798e-03],
                    [4.2092e02, 1.0015e-02],
                    [4.4084e02, 1.7013e-02],
                    [4.4326e02, 1.4318e-01],
                    [4.5521e02, 3.7168e-02],
                    [4.5819e02, 1.1827e-02],
                    [4.5886e02, 9.7405e-03],
                    [4.7523e02, 9.5406e-03],
                    [4.7821e02, 6.7267e-02],
                    [4.8823e02, 9.8752e-03],
                    [4.8857e02, 3.0635e-02],
                    [4.9457e02, 7.7622e-03],
                    [5.2924e02, 3.3289e-03],
                    [5.4431e02, 2.7525e-01],
                    [5.5525e02, 1.7059e-01],
                    [5.5725e02, 1.1219e-02],
                    [5.6125e02, 1.8678e-02],
                    [6.0027e02, 3.5606e-02],
                    [6.2629e02, 6.0038e-02],
                    [6.5534e02, 5.5606e-02],
                    [6.5629e02, 6.7757e-02],
                    [6.6933e02, 9.7967e-02],
                    [6.7335e02, 2.0507e-01],
                    [6.9283e02, 6.3219e-04],
                    [6.9733e02, 1.8395e-01],
                    [7.0133e02, 8.5427e-02],
                    [7.0182e02, 1.6290e-02],
                    [7.0933e02, 1.5958e-02],
                    [7.2733e02, 2.7861e-01],
                    [7.2932e02, 1.7530e-02],
                    [7.3032e02, 5.6805e-03],
                    [7.4085e02, 1.5080e-02],
                    [7.4136e02, 1.9213e-02],
                    [7.4987e02, 7.3668e-02],
                    [7.5434e02, 1.1365e-02],
                    [7.7037e02, 3.5787e-02],
                    [7.8037e02, 8.2898e-02],
                    [7.8438e02, 3.9444e-02],
                    [7.9837e02, 3.5896e-01],
                    [8.0239e02, 1.2173e-01],
                    [8.2636e02, 1.9253e-01],
                    [8.7343e02, 1.3567e-02],
                    [9.2741e02, 3.5401e-01],
                    [9.3891e02, 1.1278e-02],
                    [9.5541e02, 1.8427e-02],
                    [1.0385e03, 9.7895e-02],
                    [1.0564e03, 1.6603e-01],
                    [1.1395e03, 1.0147e-01],
                    [1.1985e03, 1.5947e-02],
                    [1.1996e03, 1.4755e-02],
                    [1.3836e03, 3.1716e-02],
                    [1.4807e03, 1.2714e-01],
                    [1.4997e03, 7.9945e-02],
                ],
                [
                    [1.0373e02, 1.9172e-02],
                    [1.1295e02, 1.9775e-02],
                    [1.2709e02, 1.9514e-02],
                    [1.2909e02, 2.0092e-02],
                    [1.2910e02, 4.2484e-01],
                    [1.3009e02, 4.5766e-02],
                    [1.3464e02, 1.8154e-02],
                    [1.4711e02, 1.3013e-01],
                    [1.4905e02, 4.4450e-02],
                    [1.6706e02, 1.2789e-01],
                    [1.6806e02, 2.4812e-02],
                    [1.7309e02, 2.7503e-02],
                    [1.8711e02, 9.7907e-01],
                    [2.0109e02, 3.3382e-02],
                    [2.0257e02, 2.1820e-02],
                    [2.1510e02, 2.1673e-01],
                    [2.1815e02, 1.8551e-01],
                    [2.2306e02, 3.3982e-02],
                    [2.2912e02, 2.3410e-02],
                    [2.4009e02, 2.5321e-02],
                    [2.4013e02, 9.2164e-02],
                    [2.8105e02, 2.8428e-02],
                    [2.8614e02, 4.0714e-02],
                    [2.9906e02, 2.1757e-01],
                    [3.0206e02, 3.1681e-01],
                    [3.0267e02, 3.1126e-02],
                    [3.4697e02, 3.1787e-02],
                    [3.6103e02, 4.6136e-02],
                    [3.6202e02, 1.8448e-02],
                    [4.0494e02, 9.5060e-02],
                    [4.1703e02, 1.1564e-01],
                    [4.1803e02, 2.7509e-01],
                    [4.1903e02, 6.8065e-02],
                    [4.4610e02, 2.4100e-02],
                    [4.5823e02, 2.9791e-02],
                    [4.7623e02, 1.5054e-01],
                    [4.8118e02, 3.3757e-02],
                    [5.0193e02, 1.5327e-02],
                    [5.0611e02, 9.3709e-02],
                    [5.3826e02, 2.7820e-02],
                    [5.4028e02, 5.8191e-02],
                    [5.4112e02, 5.2359e-01],
                    [5.4126e02, 3.1409e-02],
                    [5.6310e02, 3.8856e-02],
                    [5.6509e02, 5.5621e-02],
                    [5.8632e02, 3.0693e-02],
                    [5.9331e02, 4.6881e-02],
                    [5.9912e02, 3.5314e-01],
                    [6.0030e02, 3.8406e-02],
                    [6.0111e02, 4.8966e-01],
                    [6.0282e02, 2.7721e-02],
                    [6.0433e02, 1.6484e-01],
                    [6.2932e02, 2.9505e-02],
                    [6.5783e02, 9.8732e-02],
                    [6.6683e02, 4.0490e-02],
                    [6.8730e02, 4.4237e-02],
                    [7.1741e02, 4.2119e-01],
                    [7.8636e02, 2.8246e-02],
                    [8.7938e02, 3.2445e-02],
                    [9.0138e02, 1.1905e-01],
                    [9.4746e02, 1.1268e-01],
                    [1.0585e03, 3.3331e-02],
                    [1.1384e03, 4.2251e-02],
                    [1.1420e03, 3.7654e-02],
                    [1.1491e03, 2.8410e-02],
                    [1.2036e03, 3.6906e-02],
                    [1.2036e03, 2.8870e-01],
                    [1.4037e03, 1.0000e00],
                    [1.4047e03, 1.1753e-01],
                    [1.4163e03, 4.4060e-02],
                    [1.4409e03, 4.4194e-02],
                    [1.5028e03, 8.7450e-02],
                    [1.6178e03, 4.5380e-01],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                    [0.0000e00, 0.0000e00],
                ],
            ]
        ),
        rtol=1e-04,
    )
    assert torch.equal(
        spectra_padding_mask,
        torch.tensor(
            [
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                ],
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                ],
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                ],
            ]
        ),
    )
    assert torch.allclose(
        precursors,
        torch.tensor(
            [
                [1598.7540, 2.0000, 800.3843],
                [1598.7551, 3.0000, 533.9257],
                [1616.7905, 3.0000, 539.9374],
            ]
        ),
    )
    assert torch.allclose(
        peptides,
        torch.tensor(
            [
                [
                    10,
                    18,
                    10,
                    5,
                    1,
                    11,
                    12,
                    10,
                    1,
                    7,
                    8,
                    2,
                    4,
                    1,
                    13,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                ],
                [
                    6,
                    11,
                    18,
                    4,
                    1,
                    14,
                    2,
                    2,
                    14,
                    14,
                    6,
                    2,
                    2,
                    4,
                    1,
                    13,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                ],
                [
                    11,
                    5,
                    2,
                    14,
                    13,
                    12,
                    11,
                    11,
                    8,
                    13,
                    14,
                    14,
                    2,
                    13,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                ],
            ]
        ),
    )
    assert torch.allclose(
        peptide_padding_mask,
        torch.tensor(
            [
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                ],
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                ],
                [
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    False,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                    True,
                ],
            ]
        ),
    )

    device = "cuda" if torch.cuda.is_available() else "cpu"
    spectra = spectra.to(device)
    spectra_padding_mask = spectra_padding_mask.to(device)
    precursors = precursors.to(device)
    peptides = peptides.to(device)
    peptide_padding_mask = peptide_padding_mask.to(device)

    with torch.no_grad():
        reset_seed()
        pred_aa, probs = diffusion_decoder.decode(
            spectra=spectra,
            spectra_padding_mask=spectra_padding_mask,
            precursors=precursors,
            initial_sequence=peptides,
        )

    preds = ["".join(pep) for pep in pred_aa]
    if device == "cuda":
        assert preds == ["DTEDRVQPGGYAPGK", "TPARDEAAEETAAPGK", "DVAEKQDDIKEEAK"]
        assert np.allclose(
            probs, [-1.7420989274978638, -0.07612105458974838, -0.01360385213047266], rtol=1e-4
        )
    else:
        assert preds == ["NGSRWVM(+15.99)VGQDAPGK", "TPAREDAAEETAAPGK", "DVAEKQDDIKEEAK"]
        assert np.allclose(
            probs, [-1.3853957653045654, -0.04906697943806648, -0.005087343044579029], rtol=1e-4
        )

    model, config = instanovo_model
    metrics = Metrics(config["residues"], config["isotope_error_range"])
    s2i = {v: k for k, v in model.i2s.items()}

    transformer_dataset = SpectrumDataset(dataset, s2i, config["n_peaks"], return_str=True)

    transformer_dataloader = DataLoader(
        transformer_dataset, batch_size=3, shuffle=False, collate_fn=collate_batch
    )

    transformer_batch = next(iter(transformer_dataloader))
    _, _, _, targ_peptides, _ = transformer_batch

    assert targ_peptides == ("TPGREDAAEETAAPGK", "TPGREDAAEETAAPGK", "DVAEKQDDIKEEAK")

    aa_precision, aa_recall, peptide_recall, peptide_precision = metrics.compute_precision_recall(
        targets=targ_peptides, predictions=preds
    )
    aa_error_rate = metrics.compute_aa_er(targ_peptides, preds)
    auc = metrics.calc_auc(targ_peptides, preds, np.exp(pd.Series(probs)))

    if device == "cuda":
        assert math.isclose(aa_error_rate, 0.30434782608695654, abs_tol=1e-05)
        assert math.isclose(aa_precision, 0.35555555555555557, abs_tol=1e-05)
        assert math.isclose(aa_recall, 0.34782608695652173, abs_tol=1e-05)
        assert math.isclose(peptide_precision, 0.3333333333333333, abs_tol=1e-05)
        assert math.isclose(peptide_recall, 0.3333333333333333, abs_tol=1e-05)
        assert math.isclose(auc, 0.0, abs_tol=1e-05)

    else:
        assert math.isclose(aa_error_rate, 0.2608695652173913, abs_tol=1e-05)
        assert math.isclose(aa_precision, 0.35555555555555557, abs_tol=1e-05)
        assert math.isclose(aa_recall, 0.34782608695652173, abs_tol=1e-05)
        assert math.isclose(peptide_precision, 0.3333333333333333, abs_tol=1e-05)
        assert math.isclose(peptide_recall, 0.3333333333333333, abs_tol=1e-05)
        assert math.isclose(auc, 0.0, abs_tol=1e-05)
