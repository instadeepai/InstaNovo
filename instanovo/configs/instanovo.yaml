defaults:
  - _self_
  - model: instanovo_base
  - dataset: default
  - residues: default

run_name: instanovo
tags:
  - default

# Overwritten when using Aichor
tb_summarywriter: "./logs/instanovo/instanovo-base"

# Training parameters
seed: 101
warmup_iters: 100_000
learning_rate: 5e-5 #5e-4 5e-5
weight_decay: 0.0 #1e-5
train_batch_size: 128
grad_accumulation: 1
gradient_clip_val: 10.0
predict_batch_size: 128
lr_scheduler: "cosine"

# Data loading parameters
num_workers: 8
pin_memory: False
prefetch_factor:
use_shuffle_buffer: True
shuffle_buffer_size: 100_000

compile_model: True
force_cpu: False
fp16: True
mps: False
force_fp32: False

# Logging parameters
# epochs: 10
training_steps: 2_500_000
num_sanity_val_steps: 10
validate_before_training: True
console_logging_steps: 2000
tensorboard_logging_steps: 500
use_neptune: True

enable_verbose_logging: False
enable_verbose_accelerate: True
enable_verbose_s3: True

# Training data parameters
train_subset: 1.0
valid_subset: 1.0

# Number of model updates between validations
validation_interval: 100_000 # roughly half an epoch

# Checkpointing parameters
# Both model and accelerator level checkpointing
# Either can save every `checkpoint_interval` or only keep best and latest

# Model level checkpointing
save_model: True
# If false, only keep the best checkpoint and latest checkpoint
# Best checkpoint is the one with the lowest `metric_to_keep_model`
# If true, save every `checkpoint_interval`
keep_model_every_interval: False
# Accelerator level checkpointing
save_accelerator_state: True
# If false, only keep the best checkpoint and latest checkpoint
# If true, save every `checkpoint_interval`
keep_accelerator_every_interval: False
# Other checkpointing parameters
model_save_folder_path: "checkpoints/instanovo-base"
# Number of model updates between checkpoints
checkpoint_interval: 100_000 # roughly half an epoch
# Metric to keep the best checkpoint, eg
checkpoint_metric: "valid_loss"
# Min or max, min means lower is better, max means higher is better
checkpoint_metric_mode: "min"

# Accelerate checkpoint (resume training)
resume_accelerator_state:
# Model checkpoint (fine-tuning)
resume_checkpoint_path:
